# EdgeLite â€“ Edge AI Engagement Monitor & Model Optimizer

A comprehensive edge AI suite for **real-time student engagement monitoring** and **PyTorch model optimization**. Built with on-device inference (MediaPipe, no cloud), MJPEG video streaming, and a modern React dashboard.

---

## ğŸ¯ Features

### ğŸ“Š **Student Engagement Analyzer**
- **Real-time engagement metrics** via on-device MediaPipe processing
- **Eye tracking** â€“ EAR (Eye Aspect Ratio), eye contact, blink detection
- **Head pose analysis** â€“ tilt angle, head nods, head shakes
- **Engagement scoring** â€“ attention level and confusion detection
- **Live MJPEG video feed** with annotated frame from backend
- **Interactive dashboard** with timeline charts, pie charts, and bar charts
- **Session reports** â€“ aggregated metrics over time
- **3-second polling** to balance performance and data freshness
- **CSV logging** â€“ 1 entry every 3 seconds (90 frames @ 30fps)

### âš¡ **Model Optimizer**
- **PyTorch to ONNX conversion**
- **Quantization** for edge devices
- **Model compression** pipeline
- **Upload & download optimized models**

### ğŸ—ï¸ **Architecture**
- **Backend APIs** â€“ FastAPI services for engagement monitoring and optimization
- **Frontend Dashboard** â€“ React + Vite with real-time metrics visualization
- **CORS-enabled** for local development
- **Offline-first** design â€“ all processing happens on-device

---

## ğŸ“¦ Project Structure

```
EdgeLite/
â”œâ”€â”€ frontend/                      # React + Vite dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx               # Main app with navigation
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ EngagementAnalyzer.jsx    # Real-time engagement UI
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelOptimizer.jsx        # Model optimization UI
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.jsx
â”‚   â”‚   â”‚   â””â”€â”€ OptimizationMetrics.jsx
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ postcss.config.js
â”‚
â”œâ”€â”€ model1/                        # Engagement monitoring backend
â”‚   â”œâ”€â”€ api_server.py             # FastAPI server (port 8001)
â”‚   â”œâ”€â”€ pipeline.py               # EngagementPipeline class
â”‚   â”œâ”€â”€ config.py                 # Configuration (camera, FPS, paths)
â”‚   â”œâ”€â”€ ear_detector.py           # Eye Aspect Ratio detection
â”‚   â”œâ”€â”€ eye_contact.py            # Eye contact tracking
â”‚   â”œâ”€â”€ engagement.py             # Engagement scoring
â”‚   â”œâ”€â”€ confusion.py              # Confusion detection
â”‚   â”œâ”€â”€ gesture.py                # Gesture recognition
â”‚   â”œâ”€â”€ head_pose.py              # Head pose estimation
â”‚   â”œâ”€â”€ logger.py                 # CSV logging
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ face_landmarker.task      # MediaPipe model
â”‚   â””â”€â”€ logs/                     # CSV session logs
â”‚
â”œâ”€â”€ optimization-service/          # Model optimization backend
â”‚   â”œâ”€â”€ main.py                   # FastAPI server (port 8000)
â”‚   â”œâ”€â”€ optimizer.py              # ONNX conversion & quantization
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ uploads/                  # Input PyTorch models
â”‚   â””â”€â”€ optimized/                # Output ONNX models
â”‚
â”œâ”€â”€ edgeopt-backend/              # Node.js backend
â”‚   â”œâ”€â”€ server.js                 # Express server (port 5000)
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ routes/
â”‚   â””â”€â”€ models/
â”‚
â”œâ”€â”€ sample_8mb_model.pt           # Sample PyTorch model
â”œâ”€â”€ sample_128mb_model.pt          # Large sample model
â”œâ”€â”€ generate_model.py             # Script to generate test models
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.9+** (for backends)
- **Node.js 16+** (for frontend & edgeopt-backend)
- **Webcam** (for engagement monitoring)

### 1ï¸âƒ£ Setup & Run Engagement Analyzer Backend

```bash
cd model1
python -m venv venv
source venv/Scripts/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
uvicorn api_server:app --host 0.0.0.0 --port 8001
```

The backend will:
- Open your webcam
- Run MediaPipe face detection
- Stream MJPEG video to `/video_feed`
- Expose metrics on `/metrics`
- Log session data to `logs/session_*.csv`

**Endpoints:**
- `POST /session/start` â€“ Start monitoring
- `POST /session/stop` â€“ Stop monitoring
- `GET /metrics` â€“ Get all metrics (JSON)
- `GET /video_feed` â€“ MJPEG stream
- `GET /session/status` â€“ Session status

### 2ï¸âƒ£ Setup & Run Frontend Dashboard

```bash
cd frontend
npm install
npm start
```

Opens at [http://localhost:5173](http://localhost:5173)

**Features:**
- Start/stop engagement capture
- Live video feed + real-time metrics
- Timeline charts (engagement over time)
- Pie chart (attention distribution)
- Bar chart (score breakdown)
- Detailed signals (eye metrics, head gestures)
- Session reports

### 3ï¸âƒ£ (Optional) Model Optimization Service

```bash
cd optimization-service
python -m venv venv
source venv/Scripts/activate

pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --port 8000
```

Or:
```bash
cd edgeopt-backend
npm install
npm start
```

---

## ğŸ“Š Dashboard Views

### **Live Dashboard**
- Real-time video feed with MJPEG stream
- Engagement/Confusion scores with progress bars
- Eye attention metrics (EAR, eye contact, blinking)
- Head pose data (tilt, nods, shakes)
- Blink activity counter
- System performance stats (FPS, inference time, frame count)
- Real-time engagement timeline
- State distribution pie chart
- Score breakdown bar chart

### **Detailed Signals**
- Eye metrics panel (EAR left/right, eye contact ratio, blinking, sleeping)
- Head & gesture panel (tilt angle, head nodes, shakes, blink count)

### **Session Report**
- Aggregated statistics (samples collected, avg engagement, avg confusion)
- Historical timeline visualization

### **System Metrics**
- FPS, Inference time, Frames processed
- Face detection status
- Pipeline health

---

## ğŸ”§ Configuration

Edit [model1/config.py](model1/config.py) to customize:

```python
CAMERA_INDEX = 0                  # Webcam index
FRAME_WIDTH = 640                 # Resolution
FRAME_HEIGHT = 480
TARGET_FPS = 30                   # Target frame rate

LOG_INTERVAL_FRAMES = 90          # CSV log every ~3 seconds
LOG_DIR = "logs"
LOG_FILENAME = "session_log.csv"

# Gesture detection thresholds
GESTURE_DEADZONE = 0.008
```

---

## ğŸ“ˆ How It Works

### **Engagement Pipeline** (model1/pipeline.py)

1. **Capture** â€“ OpenCV reads webcam frames at 30fps
2. **Face Detection** â€“ MediaPipe detects face landmarks (468 points)
3. **Extract Signals**:
   - Eye Aspect Ratio (EAR) â†’ blinking & sleepiness
   - Eye contact â†’ gaze direction
   - Head pose â†’ tilt angle
   - Gestures â†’ head nods, shakes
4. **Score** â€“ Engagement & confusion metrics (0-100)
5. **Stream** â€“ MJPEG encode & serve to frontend
6. **Log** â€“ CSV entry every 3 seconds with all metrics

### **API Response** (/metrics)

```json
{
  "face_detected": true,
  "fps": 30.5,
  "inference_ms": 12.3,
  "frame_number": 1542,
  "ear_left": 0.25,
  "ear_right": 0.26,
  "ear_avg": 0.255,
  "is_blinking": false,
  "is_sleeping": false,
  "blink_count": 47,
  "blinks_per_minute": 15.7,
  "tilt_angle_deg": 5.2,
  "is_tilted": false,
  "eye_contact": true,
  "contact_ratio": 0.92,
  "engagement_score": 75.3,
  "confusion_score": 2.1,
  "head_nod": false,
  "head_shake": false
}
```

---

## ğŸ“ Logging

Session data is logged to CSV every 3 seconds:

**File:** `model1/logs/session_YYYYMMDD_HHMMSS.csv`

**Columns:**
- timestamp, frame_number, sleep_state, tilt_state, tilt_angle_deg
- blink_count, blinks_per_minute, eye_contact, engagement_score, confusion_score
- head_nod, head_shake, ear_left, ear_right, ear_avg, contact_ratio

---

## ğŸ¨ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Frontend** | React 18, Vite, Tailwind CSS, Recharts |
| **Engagement Backend** | FastAPI, OpenCV, MediaPipe, NumPy |
| **Optimization Backend** | FastAPI, ONNX, PyTorch |
| **Node Backend** | Express.js |
| **Streaming** | MJPEG over HTTP |
| **Logging** | CSV |

---

## ğŸ”Œ API Endpoints

### **Engagement Analyzer (Port 8001)**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check |
| `POST` | `/session/start` | Start camera capture & pipeline |
| `POST` | `/session/stop` | Stop pipeline |
| `GET` | `/session/status` | Get session status |
| `GET` | `/metrics` | Get all metrics (JSON) |
| `GET` | `/video_feed` | MJPEG stream |

### **Model Optimizer (Port 8000)**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/optimize` | Upload & optimize PyTorch model |

---

## ğŸ› Troubleshooting

### **Camera Not Opening**
- Check `CAMERA_INDEX` in [config.py](model1/config.py)
- Grant webcam permissions

### **No Video Feed**
- Confirm api_server is running on port 8001
- Check CORS settings in api_server.py
- Ensure camera is not in use by another app

### **Slow Metrics Polling**
- Polling interval is 3000ms (3 seconds) â€“ change in EngagementAnalyzer.jsx: `setInterval(poll, 3000)`
- CSV logs every 90 frames (~3s) â€“ adjust `LOG_INTERVAL_FRAMES` in config.py

### **High Inference Time**
- Reduce `FRAME_WIDTH` and `FRAME_HEIGHT` in config.py
- Lower `TARGET_FPS`
- Check CPU/GPU usage

---

## ğŸ“š Key Files

| File | Purpose |
|------|---------|
| [api_server.py](model1/api_server.py) | FastAPI server with MJPEG streaming |
| [pipeline.py](model1/pipeline.py) | Main EngagementPipeline class |
| [config.py](model1/config.py) | Global configuration |
| [EngagementAnalyzer.jsx](frontend/src/components/EngagementAnalyzer.jsx) | React dashboard component |
| [ear_detector.py](model1/ear_detector.py) | Eye aspect ratio detection |
| [engagement.py](model1/engagement.py) | Engagement scoring algorithm |

---

## ğŸ“ Learning Resources

- **MediaPipe Face Landmarks**: [mediapipe.dev](https://developers.google.com/mediapipe)
- **Eye Aspect Ratio (EAR)**: Tereza SoukupovÃ¡ & Jan ÄŒech, 2016
- **FastAPI**: [fastapi.tiangolo.com](https://fastapi.tiangolo.com)
- **Recharts**: [recharts.org](https://recharts.org)

---

## ğŸ“„ License

This project is part of the EdgeLite Hackathon. All code is provided as-is for educational and research purposes.

---

## ğŸ¤ Support

For issues or questions:
1. Check the troubleshooting section above
2. Review [model1/config.py](model1/config.py) for configuration details
3. Check api_server logs at port 8001
4. Ensure all Python dependencies are installed: `pip install -r requirements.txt`

---

**Built with â¤ï¸ for edge AI engagement monitoring Â· No cloud, all local.**
